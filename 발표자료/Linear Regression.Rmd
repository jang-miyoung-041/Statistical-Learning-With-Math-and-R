---
title: "Chapter 2 : Linear Regression"
author: "Newton's three sisters"
date: "February 16, 2025"
output:
  beamer_presentation:
    latex_engine: xelatex
    theme: metropolis
  slidy_presentation: default
fonttheme: serif
fontsize: 8pt
institute: Department of Statistics \newline Sungshin Women’s University
header-includes: \input{header_includes.tex}
---

# Least Squares Method

## Simple Linear Regression

\begin{itemize}
    \item The data consists of $(x_1,y_1),...,(x_N,y_N)$
    $$
    {y_i} = \beta_0+\beta_1{x_i}+ \varepsilon_i
    $$
    \begin{itemize}
        \item $\beta_0$: intercept
        \item $\beta_1$: slope
        \item $\varepsilon_i$: random error
    \end{itemize}
\end{itemize}

- We obtain $\beta_0$ and $\beta_1$ via the least squares method.


## Least Squares Method

- sum of squares of the residuals,\newline we minimize $L$ of the squared distances $L$ between $(x_i,y_i)$ and $(x_i,\beta_0 + \beta_1{x_i})$ over $i = 1,2, ...,N$.
\begin{align*}
L=\sum_{i=1}^N(y_i-\beta_0-\beta_1{x_i})^2
\end{align*}

- Then, by partially differentiating $L$ by $\beta_0,\beta_1$ and letting them be zero.
\begin{align*}
\frac{\partial L}{ \partial\beta_0} &=-2\sum_{i=1}^N(y_i-\beta_0-\beta_1{x_i})=0 \\
\frac{\partial L}{ \partial\beta_1} &= -2\sum_{i=1}^N(x_i(y_i-\beta_0-\beta_1{x_i}))=0
\end{align*}
- $\beta_0$ and $\beta_1$ are regarded as constants when differentiating $L$ by $\beta_1$ and $\beta_0$.

## Least Squares Method

- When $\sum_{i=1}^N(x_i- \bar x)^2$ $\ne$ 0,
\newline $\hat\beta_0$, $\hat\beta_1$ instead of $\beta_0$, $\beta_1$ which means that they are not the true values but rather estimates obtained from data.
\begin{align*}
\hat\beta_1 &=\frac{\sum_{i=1}^N(x_i- \bar x)(y_i- \bar y)}{\sum_{i=1}^N(x_i- \bar x)^2} \\
\hat\beta_0 &=\bar y - \hat\beta_1{\bar x}
\end{align*}

- We center the data as follows,
\begin{align*}
\tilde{x}_1 := x_1 - \bar x, \cdots,\tilde{x}_N := x_N- \bar x, \tilde{y}_1 := y_1 - \bar y, \cdots,\tilde{y}_N := y_N- \bar y
\end{align*}

- Center the data results in a zero average.

- The formula for calculating the slope from the centralized data is as follows:
\begin{align*}
\hat\beta_1 &=\frac{\sum_{i=1}^N \tilde{x}_i\tilde{y}_i}{\sum_{i=1}^N(\tilde{x}_i)^2}
\end{align*}

## Example

- The two lines $l$ is obtained from the $N$ pairs of data and the least squares method, and  $l'$ obtained by shifting $l$ so that it goes through the origin.

```{r,echo=TRUE,eval=FALSE}
min.sq=function(x,y){
  x.bar=mean(x);y.bar=mean(y)
  beta.1=sum((x-x.bar)*(y-y.bar))/sum((x-x.bar)^2);beta.0=y.bar-beta.1*x.bar
  return(list(a=beta.0,b=beta.1))
}
a=rnorm(1);b=rnorm(1);
N=100;x=rnorm(N);y=a*x+b+rnorm(N)
plot(x,y);abline(h=0);abline(v=0)
abline(min.sq(x,y)$a,min.sq(x,y)$b,col="red")
x=x-mean(x);y=y-mean(y)
abline(min.sq(x,y)$a,min.sq(x,y)$b,col="blue")
legend("topleft",c("BEFORE","AFTER"),lty=1,col=c("red","blue"))
```

## Example
```{r,echo=FALSE}
set.seed(60)
min.sq=function(x,y){
  x.bar=mean(x);y.bar=mean(y)
  beta.1=sum((x-x.bar)*(y-y.bar))/sum((x-x.bar)^2);beta.0=y.bar-beta.1*x.bar
  return(list(a=beta.0,b=beta.1))
}
a=rnorm(1);b=rnorm(1);
N=100;x=rnorm(N);y=a*x+b+rnorm(N)
plot(x,y);abline(h=0);abline(v=0)
abline(min.sq(x,y)$a,min.sq(x,y)$b,col="red")
x=x-mean(x);y=y-mean(y)
abline(min.sq(x,y)$a,min.sq(x,y)$b,col="blue")
legend("topleft",c("BEFORE","AFTER"),lty=1,col=c("red","blue"))
```

# Multiple Regression

## Multiple Regression with Matrices

We formulate the least squares method for multiple regression with matrices. 

- $L:=\sum_{i=1}^N(y_i-\beta_0-\beta_1{x_i})^2$,
\begin{align*}
L = \parallel y-X\beta \parallel^2 = (y-X\beta)^T(y-X\beta)
\end{align*}

- If we define,
\begin{align*}
y :=
\begin{bmatrix}
y_1 \\
\vdots \\
y_N
\end{bmatrix}
, X :=
\begin{bmatrix}
1 & x_{1,1} &\cdots &x_{1,p}\\
\vdots & \vdots &\ddots &\vdots \\
1 & x_{N,1} &\cdots &x_{N,p}
\end{bmatrix}
, \beta :=
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p
\end{bmatrix}
\end{align*}

- Partial differentiation with $L$
\begin{align*}
\nabla L := 
\begin{bmatrix}
\frac{\partial L}{\partial\beta_0} \\
\frac{\partial L}{\partial\beta_1}
\end{bmatrix}
= -2X^T(y-X\beta)
\end{align*}

## Multiple Regression

- Set to zero to find the minimum value
\begin{align*}
-2X^T(y-X\beta) =
\begin{bmatrix}
-2\sum_{i=1}^N(y_i-\sum_{j=0}^p \beta_j x_{i,j}) \\
-2\sum_{i=1}^N x_{i,1}(y_i-\sum_{j=0}^p \beta_j x_{i,j}) \\
\vdots \\
-2\sum_{i=1}^N x_{i,p}(y_i-\sum_{j=0}^p \beta_j x_{i,j})
\end{bmatrix}
\end{align*}

- When a matrix $X^TX$ is invertible, we have
\begin{align*}
\hat\beta= (X^TX)^{-1} X^Ty
\end{align*}


## When $X^TX$ is irreversible
1. $N < p+1$
\begin{align*}
rank(X^TX) \le rank(X) \le min \{N, p+1\} = N < p+1
\end{align*}
If $N > p$, It is $X$_particular, So there is no inverse matrix. 

2. Two columns in $X$ coincide.
\begin{align*}
X^TX z=0 \Rightarrow z^TX^TX_Z=0 \Rightarrow \parallel X_z \parallel^2 =0 \Rightarrow X_z=0
\end{align*}

# Distribution of $\hat\beta$

## Distribution of $\hat\beta$ 제목 생각해보기
- $y$ have been obtained from the covariates $X$ multiplied by the (true) coefficients $\beta$ plus some noise $\epsilon$.
\begin{align*}
y = X\beta +  \epsilon
\end{align*}

- The true $\beta$ is unknown and different from the estimate $\hat\beta$.
- We have estimated $\hat\beta$ via the least squares method from the $N$ pairs of data ($x_1,y_1$),$\cdots$,($x_N,y_N$) $\in$ $R^p$ X $R$

- $x_i$ $\in$ $R^p$ is the row vector consisting of $p$ values excluding the leftmost one in the $i$th row of $X$. 


## Density function
- We assume that each element $\epsilon_1,\cdots,\epsilon_N$ in the random variable $\epsilon$ is independent of the others and Gaussian distribution with mean zero and variance $\sigma^2$. $N(0,\sigma^2)$
\begin{align*}
f_i(\epsilon_i) = \frac{1}{\sqrt {2\pi\sigma^2}} e^{-\frac{\epsilon_i^2}{2\sigma^2}}
\end{align*}

- We may express the distributions of $\epsilon_1,\cdots,\epsilon_N$ by
\begin{align*}
f(\epsilon) = \prod_{i=1}^N f_i(\epsilon_i) = \frac{1}{(2\pi\sigma^2)^{N/2}} e^{-\frac{\epsilon^T \epsilon}{2\sigma^2}}
\end{align*}
This is $N(0,\sigma^2I)$, $I$ is a unit matrix of size $N$.


## Independent if and only if their covariance is zero

- For the proof,
\begin{align*}
\hat\beta = (X^TX)^{-1}X^T(X\beta+\epsilon) = \beta + (X^TX)^{-1}X^T\epsilon
\end{align*}

- Since the average of $\epsilon \in R^N$ is zero, the average of $\epsilon$ multiplied from left by the constant matrix $(X^TX)^{-1}X^T$ is zero.
\begin{align*}
E[\hat\beta] = \beta
\end{align*}

- In general, we say that an estimate is unbiased if its average coincides with the true value.

## Covariance matrix of $\hat\beta$

- $\hat\beta$ and its average $\beta$ consist of $p+1$ values. 
- $V(\hat\beta_i) = E(\hat\beta_i - \beta_i)^2, i=0,1,\cdots,p$, the covariance $\sigma_{i,j} := E(\hat\beta_i - \beta_i)(\hat\beta_j - \beta_j)^T$ can be defined for each pair $i\ne j$.

- matrix consisting of $\sigma_{i,j}$ in the $i$th row and $j$th column as to the covariance matrix of $\hat\beta$.

\begin{align*}
& E
\begin{bmatrix}
(\hat\beta_0 - \beta_0)^2 & (\hat\beta_0 - \beta_0)(\hat\beta_1 - \beta_1) &\cdots &(\hat\beta_0 - \beta_0)(\hat\beta_p - \beta_p)\\
(\hat\beta_1 - \beta_1)(\hat\beta_0 - \beta_0) &(\hat\beta_1 - \beta_1)^2 &\cdots &(\hat\beta_1 - \beta_1)(\hat\beta_p - \beta_p)\\
\vdots &\vdots &\ddots &\vdots \\
(\hat\beta_p - \beta_p)(\hat\beta_0 - \beta_0) &(\hat\beta_p - \beta_p)(\hat\beta_1 - \beta_1) &\cdots &(\hat\beta_p - \beta_p)^2
\end{bmatrix} 
\end{align*}

## Covariance matrix of $\hat\beta$

\begin{align*}
& E
\begin{bmatrix}
(\hat\beta_0 - \beta_0)^2 & (\hat\beta_0 - \beta_0)(\hat\beta_1 - \beta_1) &\cdots &(\hat\beta_0 - \beta_0)(\hat\beta_p - \beta_p)\\
(\hat\beta_1 - \beta_1)(\hat\beta_0 - \beta_0) &(\hat\beta_1 - \beta_1)^2 &\cdots &(\hat\beta_1 - \beta_1)(\hat\beta_p - \beta_p)\\
\vdots &\vdots &\ddots &\vdots \\
(\hat\beta_p - \beta_p)(\hat\beta_0 - \beta_0) &(\hat\beta_p - \beta_p)(\hat\beta_1 - \beta_1) &\cdots &(\hat\beta_p - \beta_p)^2
\end{bmatrix} \\
&= E
\begin{bmatrix}
\hat\beta_0 - \beta_0 \\
\hat\beta_1 - \beta_1 \\
\vdots \\
\hat\beta_p - \beta_p
\end{bmatrix}
\begin{bmatrix}
\hat\beta_0 - \beta_0, \hat\beta_1 - \beta_1, \cdots, \hat\beta_p - \beta_p
\end{bmatrix} \\
&= E(\hat\beta-\beta)(\hat\beta-\beta)^T = E(X^TX)^{-1}X^T \epsilon{(X^TX)^{-1}X^T\epsilon}^T \\
&= (X^TX)^{-1}X^T E \epsilon\epsilon^T X(X^TX)^{-1} = \sigma^2(X^TX)^{-1}
\end{align*}

We have determined that the covariance matrix of $\epsilon$ is $E\epsilon\epsilon^T =\sigma^2I$.

\begin{align*}
\hat\beta \sim N(\beta,\sigma^2(X^TX)^{-1})
\end{align*}

# Distribution of the RSS Values

## Hat matrix
- Hat matrix defined by $\hat{y} = Hy$


  $\hat{y} = X\hat{\beta} = X(X^TX)^{-1}{X^T}y = Hy$
$$H \triangleq X(X^TX)^{-1}{X^T}$$

- Some properties
\begin{align*}
H^2 &= X(X^TX)^{-1}{X^T} \cdot X(X^TX)^{-1}{X^T} = X(X^TX)^{-1}{X^T} = H \\
(I -H)^2 &= I - 2H + H^2 = I - H \\
HX &= X(X^TX)^{-1}{X^T} \cdot X = X
\end{align*}


## Residual Sum of Square
- RSS defined
$$
\text{RSS} \triangleq ||y-\hat{y}||^2
$$
- Using hat matrix
\begin{align*}
y - \hat{y} &= y - Hy = (I-H)y = (I-H)(X\beta+\varepsilon) \\
            &= (X -HX)\beta + (I - H)\varepsilon = (I - H)\varepsilon
\end{align*}
$$
RSS \triangleq ||y - \hat{y}||^2 = \{(I - H)\varepsilon\}^T (I - H)\varepsilon = \varepsilon^T(I-H)^2\varepsilon = \varepsilon^T(I-H)\varepsilon
$$

## Eigenvalues of $H$ and Null space of ($I-H$)
- Proof by contrapositive
\begin{align*}
Hx = x \Rightarrow (I - H)x = 0 \\
(I - H)x = 0 \Rightarrow Hx = x
\end{align*}
- Dimensions of the eigenspaces of $H$ is $p + 1$ \
\vt
\textbf{Proof} using rank($X) = p+1$ 
\begin{align*}
\text{rank}(H) &\le \text{min}\{\text{rank}(X(X^TX)^{-1}), \text{rank}(X)\} \le \text{rank}(X) = p + 1 \\
\text{rank}(H) &\ge \text{rank}(HX) = \text{rank}(X) = p + 1
\end{align*}

- Dimensions of the null space of $I-H$ is $N - (p+1)$

$$
P (I - H) P^T = \operatorname{diag}(\underbrace{1, \dots, 1}_{N-p-1}, \underbrace{0, \dots, 0}_{p+1})
$$

## Residual Sum of Squares (RSS) and Eigenvalue
-  We define $v = P\varepsilon \in \mathbb{R}^N$, then from $\varepsilon = P^Tv$
\begin{align*}
\text{RSS} 
&= \varepsilon^T(I - H)\varepsilon = (P^Tv)^T(I-H)P^Tv = v^TP(I-H)P^Tv \\
&= [v_1, \cdots, v_{N-p-1}, v_{N-p}, \cdots, v_n] 
\begin{bmatrix}
1      & 0      &\cdots  &  \cdots & \cdots & 0 \\
0      & \ddots & 0      &  \cdots & \cdots & \vdots \\
\vdots & 0      & 1      &  \cdots & \cdots & 0 \\
0      & 0      & 0      &  \cdots & \cdots & \cdots \\
\vdots & \vdots & \vdots &  \vdots & \ddots & \vdots \\
0      & \cdots & 0      &  \cdots & \cdots & 0
\end{bmatrix}
\begin{bmatrix}
v_1 \\
\vdots \\
v_{N-p-1} \\
v_{N-p} \\
\vdots \\
v_N
\end{bmatrix} \\
&= \sum^{N-p-1}_{i = 1}v_i^2
\end{align*}

## Statistical Properties of the Residual Sum of Squares (RSS)
- Let $w \in \mathbb{R}^{N-p-1}$ be
\vt
  - Average 

    $E[v] = E[P\varepsilon] = 0$

    $E[w] = 0$
\vt

  - Covariance 

    $E[vv^t] = E[P\varepsilon(P\varepsilon)^T] = PE[\varepsilon\varepsilon^t]P=P\sigma^2IP^T = \sigma^2I$
  
    $E[ww^T] = \sigma^2I$
\vt

- We have RSS
$$
\frac{RSS}{\sigma^2} \sim \chi^2_{N-p-1}
$$

# Hypothesis Testing for $\hat{\beta}_j \ne 0$

## Test statistic $T$
- A t distribution with $N - P -1$ degrees of freedom
\vt
- We decide that hypothesis $\beta_j = 0$ should be rejected.
\vt
- $U \sim N(0,1),\ V \sim \chi^2_m$, 

$$
T \triangleq U / \sqrt{V/m}
$$

```{r echo=FALSE, fig.height=5, fig.width=9, message=FALSE}
curve(dnorm(x), -10, 10, ann = FALSE, ylim = c(0, 0.5), lwd = 5)
for(i in 1:10)curve(dt(x, df= i), -10, 10, col = i, add = TRUE, ann = FALSE)
legend("topright", legend = 1:10, lty = 1, col = 1:10)
```

## Significance level
- $\alpha = 0.01,\ 0.05$
\vt
- Null hypothesis $\beta_j = 0$

```{r fig.height=5, fig.width=9, echo=FALSE}
# 기본 정규분포 그래프
curve(dnorm(x), -6, 6, ann = FALSE, ylim = c(0, 0.6), lwd = 3)

# 유의수준(alpha) 설정
alpha <- 0.05  # 95% 신뢰구간 (5% 유의수준)
z_crit <- qnorm(1 - alpha/2)  # 양측 검정의 임계값

# 기각영역 채우기 (좌측, 우측)
x_reject_left <- seq(-6, -z_crit, length=100)
x_reject_right <- seq(z_crit, 6, length=100)

polygon(c(x_reject_left, rev(x_reject_left)), 
        c(dnorm(x_reject_left), rep(0, length(x_reject_left))), 
        col="red", border=NA)

polygon(c(x_reject_right, rev(x_reject_right)), 
        c(dnorm(x_reject_right), rep(0, length(x_reject_right))), 
        col="red", border=NA)

# 채택영역 채우기 (가운데 부분)
x_accept <- seq(-z_crit, z_crit, length=100)
polygon(c(x_accept, rev(x_accept)), 
        c(dnorm(x_accept), rep(0, length(x_accept))), 
        col="blue", border=NA)

# 텍스트 추가
text(0, 0.25, expression(1 - alpha), col = "black", cex=1.5)
text(-4, 0.05, expression(alpha/2), col = "black", cex=1.2)
text(4, 0.05, expression(alpha/2), col = "black", cex=1.2)
text(-4, 0.1, "REJECT", col="red", cex=1.2)
text(4, 0.1, "REJECT", col="red", cex=1.2)
text(0, 0.4, "ACCEPT", col="black", cex=1.5)

# 수직선 추가 (임계값)
abline(v = c(-z_crit, z_crit), col="red", lwd=2)

# 범례 추가
legend("topright", legend=c("Standard Normal", "Reject Region", "Accept Region"),
       col=c("black", "red", "blue"), lty=1, fill=c(NA, "red", "blue"), border=NA)

```

## Example 23
- For $p=1$, since
$$
X^TX = \begin{bmatrix} 1 & \cdots   & 1 \\
                       x_1 & \cdots & x_N
       \end{bmatrix}
       \begin{bmatrix}
       1 & x_1 \\
       \vdots & \vdots \\
       1 & x_n
       \end{bmatrix}
       = N \begin{bmatrix}
            1 & \bar{x} \\
            \bar{x} & \frac{1}{N}\sum_{i = 1}^{N}x_i^2
           \end{bmatrix}
$$
- The inverse is
$$
(X^TX)^{-1} = \frac{1}{N}\begin{bmatrix}
                           1      & \bar{x} \\
                          \bar{x} & \frac{1}{N}\sum_{i = 1}^{N}x_i^2
                         \end{bmatrix}^{-1} = \frac{1}{\sum_{i=1}^{N}(x_i - \bar{x})^2}
                         \begin{bmatrix}
                            \frac{1}{N}\sum_{i = 1}^{N}x_i^2       & -\bar{x} \\
                            -\bar{x}                               & 1
                         \end{bmatrix}
$$

- Which means that
$$
B_0 = \frac{\frac{1}{N}\sum_{i = 1}^{N}x_i^2}{\sum_{i=1}^{N}(x_i - \bar{x})^2} \ \ \ \text{and} \ \ \ B_1 = \frac{1}{\sum_{i=1}^{N}(x_i - \bar{x})^2}
$$

## Example 23 (contd.)
- For $B = (X^TX)^{-1}, B\sigma^2$ is covariance matrix of $\hat{\beta}$

- $B_j\sigma^2$ is the variance of $\hat{\beta}_j$

- Because $\bar{x}$ is positive, the correlation between $\hat{\beta}_0$ and $\hat{\beta}_1$ is negative
$$
t = \frac{\hat{\beta}_j - \beta_j}{\text{SE}(\hat{\beta}_j)} \sim t_{N-p-1}
$$

## Statistical Independence in Regression
- It remains to be shown that $U$ and $V$ are independent
$$
U \triangleq \frac{\hat{\beta}_j-\beta_j}{\sqrt{B_j}\sigma} \sim N(0,1) \ \ \ \text{and} \ \ \ V \triangleq \chi^2_{N-p-1}
$$

- Sufficient to show tha $y - \hat{y}$ and $\hat{\beta} - \beta$ are independent
$$
(\hat{\beta} - \beta)(y-\hat{y})^T = (X^TX)^{-1}X^T\varepsilon\varepsilon^T(I-H)
$$

- From $E\varepsilon\varepsilon^T =\sigma^2I$ and $HX = X$,
$$
E(\hat{\beta} -\beta)(y-\hat{y})^T = 0
$$

# Coefficient of Determination and the Detection of Collinearity

## RSS, ESS, and TSS

- We define a matrix $W \in \mathbb{R}^{N \times N}$ such that all the elements are $1/N$ \

  $Wy \in \mathbb{R}^N$ are $\bar{y} = Wy = \sum^{N}_{i=1}y_i$ for $y_1, \cdots,y_N \in \mathbb{R}$

- Residual sum of squares RSS
$$
\text{RSS} = ||\hat{y}-y||^2 = ||(I-H)\varepsilon||^2=||(I-H)y||^2
$$

- Explained sum of squres ESS
$$
\text{ESS} \triangleq ||\hat{y} - \bar{y}||^2 = ||\hat{y} - Wy||^2 = ||(H - W)y||^2
$$

- Total sum of squres TSS
$$
\text{TSS} \triangleq ||y - \bar{y}||^2 = ||(I - W)y||^2 
$$

<!-- ## 제목

- We have relation TSS = RSS + ESS \

  \textbf{Proof} -->

## Sample - based correlation

- Coefficient of determination
$$
R^2 = \frac{\text{ESS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}}
$$
- Correlation between the covariates and response
$$
\hat{\rho} \triangleq \frac{\sum^N_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum^N_{i=1}(x_i-\bar{x})^2\sum^N_{i=1}(y_i-\bar{y})^2}} 
$$

\begin{align*}
\frac{\text{ESS}}{\text{TSS}} &= \frac{\hat{\beta_1^2}||x-\bar{x}||^2}{||y-\bar{y}||^2} = \left\{ \frac{\sum^N_{i=1}(x_i-\bar{x})(y_i-\bar{y})}{\sum^N_{i=1}(x_i-\bar{x})^2} \right\}^2 \frac{\sum^N_{i=1}(x_i-\bar{x})^2}{\sum^N_{i=1}(y_i-\bar{y})^2} \\
&= \frac{\left\{ \sum^N_{i=1}(x_i-\bar{x})(y_i-\bar{y}) \right\}^2}{\sum^N_{i=1}(x_i-\bar{x})^2]\sum^N_{i=1}(y_i-\bar{y})^2} =\hat{\rho}^2
\end{align*}

## Variance Inflation Factors

- Variance inflation factors
$$
\text{VIF} \triangleq \frac{1}{1 - R^2_{X_j|X_{-j}}}
$$

- The minimum value of VIF is one, and we say that the collinearity of covariate is strong when its VIF value is large

# Confidence and Prediction Intervals

## Confidence Interval

- We have showed how to obtain the estimate $\hat{\beta}$ of $\beta \in \mathbb{R}^{p+1}$, confidence interval of $\hat{\beta}$ as follows
$$
\beta_i = \hat{\beta_i} \pm t_{N-p-1}(\alpha/2)\text{SE}(\hat{\beta_i}), \ \ \ \  \text{for} \ \   i = 0, 1, \cdots, p
$$

- Confidence interval of $x_* \hat{\beta}$ for another point $x_*\in \mathbb{R}^{p+1}$

  - The average
$$
E[x_*\hat{\beta}] = x_*E[\hat{\beta}]
$$

  - The variance
$$
V[x_*\hat{\beta}] = x_*V(\hat{\beta})x_*^T = \sigma^2x_*(X^TX)^{-1}x_*^T
$$
    
- We define 

$$
\hat{\sigma} \triangleq \sqrt{\text{RSS}/(N -p-1)}, \ \ \text{SE}(x_*\hat{\beta}) \triangleq \hat{\sigma}\sqrt{x_*(X^TX)^{-1}x_*^T}
$$

## Confidence and Prediction Intervals in Regression

- $\text{C} \sim t_{N-p-1}$
\vt
- variance in the difference between $x_*\hat{\beta}$ and $y_* \triangleq x_*\beta + \varepsilon$
$$
V[x_*\hat{\beta} - (x_*\beta + \varepsilon)] = V[x_*(\hat{\beta} - \beta)]+V[\varepsilon] = \sigma^2x_*(X^TX)^{-1}x_*^T +\sigma^2
$$

- Similarly, we can derive the following
$$
P \triangleq \frac{x_*\hat{\beta}-y_*}{\text{SE}(x_*\hat{\beta}-y_*)} = \frac{x_*\hat{\beta}-y_*}{\sigma(1+\sqrt{x_*(X^TX)^{-1}x_*^T)}} \Big/ \sqrt{\frac{\text{RSS}}{\sigma^2}\Big/(N-p-1)} \sim t_{N-p-1}
$$

- The confidence and prediction intervals
\begin{align*}
x_*\beta = x_*\hat{\beta} \pm t_{N-p-1}(\alpha/2)\hat{\sigma}\sqrt{x_*(X^TX)^{-1}x_*^T} \\
y_* = x_*\hat{\beta} \pm t_{N-p-1}(\alpha/2)\hat{\sigma}\sqrt{1+x_*(X^TX)^{-1}x_*^T}
\end{align*}


## Q & A

\begin{center}
  {\bf {\Huge Q \& A}}
\end{center}

## 

\begin{center}
  {\bf {\Huge Thank you :)}}
\end{center}

